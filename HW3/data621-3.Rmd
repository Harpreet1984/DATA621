---
title: "DATA-621 Project 3"
author: "Harpreet Shoker"
date: "01-Jul-2018"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: cerulean
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Cover Page

##### DATA621-Assignment-3
##### By - Harpreet Shoker
##### Date - 01-Jul-2018
##### University - City university of New York
##### Proffesor - Marcus Ellis
##### Abstract 

The purpose of this analysis is to build a logistic regression model that will predict whether a particular neighborhood in Boston is above or below the median crime level for the city.Our dataset includes information on 466 Boston neighborhoods. Each neighborhood has 13 potential predictor variables, and 1 response variable. The response variable is "target", which is "1" if the neighbhorhood is above the city's median crime level, and 0 if not.
We need to build various binary logistic models after preparing our data set and select the model that is best suited.
We are first loading data transforming the data and then building models using various libraries and functions.


### DATA EXPLORATION

First i m loading libraries that i will be using for the assignment

```{r,include=FALSE,warning=FALSE}
# Load required libraries
library(ggplot2)
library(pROC)
library(RCurl)
library(knitr)
library(Hmisc)
library(caret)
library(gridExtra)
```

NOw loading the data set crime training and evaluation sets  from Github
and displaying some of records to understand data better

```{r}
crime <- read.csv("https://raw.githubusercontent.com/Harpreet1984/DATA621/master/HW3/crime-training-data.csv")
crime_eval <- read.csv("https://raw.githubusercontent.com/Harpreet1984/DATA621/master/HW3/crime-evaluation-data.csv")
kable(head(crime))
```

Checking for any missing values in crime training data , if there is any missing data we need to imputate the data

```{r}
any(is.na(crime))
#examining target variable
w <- table(crime$target)
w

```

So the results here say there is no missing data.

Out Of the 13 predictor variables, 12 were numeric and 1 was caterogical. The categorical data would need to be converted in order to be approrpriately used in a generalized linear model.
Also while examining target variable we noticed 229 neighbourhoods are marked as 1 that is above the median and 237 are marked as 0.


Displaying summary of each variable to have better undertstanding
```{r}
kable(summary(crime[1:6]))
kable(summary(crime[7:12]))
```

######## Histograms of each variable
here using Hmisc library and using function hist.data.frame()

```{r}
hist.data.frame(crime)
```
we can see near normal distributions for medv and rm. With lstat,dis,rad,tax, and nox farther off. We may need to transform them. In particular we see dis,lstat, and age victim of a stride and true right skew.


Building a correlation plot for the variables to understand correlation between variables

```{r warning=FALSE, echo=FALSE}
library(corrplot)
corrplot(cor(crime), method="number")
``` 

We can see strong positive correlations in the following sets-indus, age, rad, tax, lstat, target :: nox
We can see strong negative correlations in the following sets-indus, nox, age, rad, tax, target :: dis



Creating box plots of each variable against target 
```{r warning=FALSE, echo=FALSE}

zn_bp <- ggplot(crime, aes(factor(target), zn)) + geom_boxplot()
chas_bp <- ggplot(crime, aes(factor(target), chas)) + geom_boxplot()
indus_bp <- ggplot(crime, aes(factor(target), indus)) + geom_boxplot()

nox_bp <- ggplot(crime, aes(factor(target), nox)) + geom_boxplot()

rm_bp<- ggplot(crime, aes(factor(target), rm)) + geom_boxplot()

age_bp <- ggplot(crime, aes(factor(target), age)) + geom_boxplot()

dis_bp <- ggplot(crime, aes(factor(target), dis)) + geom_boxplot()

rad_bp <- ggplot(crime, aes(factor(target), rad)) + geom_boxplot()

tax_bp <- ggplot(crime, aes(factor(target), tax)) + geom_boxplot()

ptratio_bp <- ggplot(crime, aes(factor(target), ptratio)) + geom_boxplot()

black_bp <- ggplot(crime, aes(factor(target), black)) + geom_boxplot()
lstat_bp <- ggplot(crime, aes(factor(target), lstat)) + geom_boxplot()
medv_bp <- ggplot(crime, aes(factor(target), medv)) + geom_boxplot()
grid.arrange(zn_bp,indus_bp,nox_bp,rm_bp,age_bp,dis_bp,chas_bp,rad_bp,tax_bp,ncol=3,nrow=3)
grid.arrange(ptratio_bp,black_bp,lstat_bp,medv_bp,ncol=2,nrow=2)

```


Looking at the box plots we see that in addition to fixing the distributional right skews of dis, lstat and age we should consider fixing zn, nox, tax, and rad through transformations to minimize their target variation.
 
### DATA PREPARATION

This data set has no missing values so no imputations here

Trying to make three datasets for preparation of this data

1) Original dataset with transformed [dis,lstat,age] with log

2) A log normalized dataset

3) Transform [dis,lstat,age,zn,nox,tax,rad] with both quadratic and log terms.

Here not doing any tranformation to the variables - zn,chas,target

Transforming the data
```{r}
library(mice)
crime_N = data.frame(
dis_n = log(crime$dis),
lstat_n = log(crime$lstat),
age_n = log(crime$age),
zn_n = crime$zn^2,
nox_n = crime$nox^2,
tax_n = crime$tax^2,
rad_n = crime$rad^2)
dataset1 = crime
dataset2 = log(crime)
dataset2$zn = crime$zn
dataset2$target = crime$target
dataset2$chas = crime$chas
dataset3 = cbind(crime,crime_N)
```  
 
### Build Models

##### 1.  Original dataset
This model i m including all variables this will help us understand which variables are significant and allow us to make better model.
This model will be based off of the original data - before transformed

```{r}
m1 = glm(data=crime,target~.,family=binomial)
summary(m1)
```  


This original  dataset model with all variables has an AIC of 214.18 with a sample over 400, meaning it may be an accurate metric over BIC.  
Residual deviance is 86.15 and Null deviance of almost 646. Significant variables include [rad,nox, dis,medv, ptratio].  


##### 2. Backward elimination using only significant variables

In this model we are performing backward elimination using only significant variales we got after running model 1.
Variables will be removed one by one to determine best fit model. After each variable is removed, the model will be 'ran' again - until the most optimal outputs are produced. Only the final output will be shown

```{r,echo=FALSE,warning=FALSE}
m2 <-  glm(data = crime,target ~ rad + nox + dis + medv + ptratio)
reducedm2<- step (m2, direction = "backward")
```
```{r}
summary(reducedm2)
```


This model has higher AIC value compared to model 1 but residual veviance is much lower of 46.307

##### 3. Log transformed dataset model, all features included
```{r}
m3 = glm(data=dataset2,target~.,family=binomial)
summary(m3)
``` 

Model 3 has AIC value of 232.07 and very high residual deviance value of 204


##### 4. Log and Quad transformed datase
```{r}
m4 = glm(data=dataset3,target~.,family=binomial)
summary(m4)
```  

Model 4 has AIC value of 168.92 and residual deviance of 126.92

We are choosing here model 2 because it has lower residual deviance 

Now we can run the anova() function on the model to analyze the table of deviance
```{r}
anova(reducedm2, test="Chisq")
```

Explaination of anova () is in the below section

### SELECT MODEL 

We have selected model 2 with lower residual deviance and less features.
From the anova() function the difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time.
Again, adding rad, nox and dis significantly reduces the residual deviance. The other variables seem to improve the model less. A large p-value here indicates that the model without the variable explains more or less the same amount of variation. Ultimately what you would like to see is a significant drop in deviance and the AIC.

Lets evaluate our model

As a last step, we are going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier.
Here is a ROC curve of our selected model and area under the curve:

```{r}
crime$predict <- predict(reducedm2, crime, type='response')

myroc <- roc(crime$target, crime$predict, plot=T, asp=NA,
                legacy.axes=T, main = "ROC Curve", ret="tp", col="blue")

myroc["auc"]
```
The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

### TEST DATA PREDICTIONS

```{r}
#Load Data
finalPred =predict(reducedm2, newdata=crime_eval)
finalPred = ifelse(finalPred<.5,0,1)
hist(finalPred)
write.csv(finalPred,'harpreet621-3.csv')
```
